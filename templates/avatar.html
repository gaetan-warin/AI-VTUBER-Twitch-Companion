<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Interactive Live2D Avatar</title>
    <link rel="icon" href="static/favicon.ico" type="image/x-icon">

    <script src="https://cubism.live2d.com/sdk-web/cubismcore/live2dcubismcore.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/dylanNew/live2d/webgl/Live2D/lib/live2d.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/pixi.js@6.5.2/dist/browser/pixi.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/pixi-live2d-display/dist/index.min.js"></script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.12.2/gsap.min.js"></script>
    <script src="https://cdn.socket.io/4.5.4/socket.io.min.js"></script>
    <link rel="stylesheet" href="static/style.css">
</head>
<body>
    <canvas id="canvas"></canvas>

    <div class="controls">
        <div class="input-group">
            <textarea id="text-input" placeholder="Enter text for the avatar to speak..."></textarea>
            <button id="speakBtn">Speak</button>
        </div>
    </div>

    <script>
        // WebSocket configuration
        const socket = io({
            transports: ['websocket'],
            reconnectionAttempts: 5,
            timeout: 3000
        });

        // Speech synthesis setup
        const synth = window.speechSynthesis;
        let isSpeaking = false;
        let mouthTween = null;
        let live2dModel = null;
        let voicesReady = false;

        // Live2D model configuration
        const cubism2Model = "model/shizuku/shizuku.model.json";

        // Initialize application
        (async function main() {
            // Set up PIXI application
            const app = new PIXI.Application({
                view: document.getElementById("canvas"),
                autoStart: true,
                resizeTo: window,
                backgroundAlpha: 0
            });

            try {
                // Load Live2D model
                live2dModel = await PIXI.live2d.Live2DModel.from(cubism2Model);
                app.stage.addChild(live2dModel);
                live2dModel.scale.set(0.3);
                
                // Initialize mouth parameter
                window.mouthParam = live2dModel.internalModel.parameters.find(
                    p => p.id === 'ParamMouthOpenY'
                );
                
                // Set up resize handler
                window.addEventListener('resize', updateModelPosition);
                updateModelPosition();

            } catch (error) {
                console.error('Model loading error:', error);
            }
        })();

        // Position handling
        function updateModelPosition() {
            if (!live2dModel) return;
            
            const isMobile = window.innerWidth <= 768;
            live2dModel.scale.set(isMobile ? 0.8 : 1.2);
            live2dModel.position.set(
                window.innerWidth / 2,
                window.innerHeight / 2 + 100
            );
        }

        // Speech functions
        function speak(text) {
            if (!text || !voicesReady) {
                console.log('Speech not ready');
                return;
            }

            if (isSpeaking) {
                synth.cancel();
                isSpeaking = false;
                mouthTween?.kill();
            }

            const utterance = new SpeechSynthesisUtterance(text);
            const voice = synth.getVoices().find(v => v.name.includes('Female')) || synth.getVoices()[0];

            Object.assign(utterance, {
                voice: voice,
                pitch: 1.1,
                rate: 0.95
            });

            utterance.onstart = () => {
                isSpeaking = true;
                mouthTween = gsap.to({}, {
                    duration: 0.1,
                    repeat: -1,
                    onRepeat: () => animateMouth(Math.random() * 0.5 + 0.5)
                });
            };

            utterance.onend = () => {
                isSpeaking = false;
                mouthTween?.kill();
                animateMouth(0);
            };

            synth.speak(utterance);
        }

        // Mouth animation
        function animateMouth(value) {
            if (live2dModel && window.mouthParam) {
                live2dModel.internalModel.setParameterValue(window.mouthParam, value);
            }
        }

        // Event listeners
        function setupEventListeners() {
            // Speech button handler
            document.getElementById('speakBtn').addEventListener('click', () => {
                const text = document.getElementById('text-input').value.trim();
                if (text) socket.emit('speak', { text });
            });

            // Socket.io events
            socket.on('speak_text', data => {
                console.log('Received speech text:', data.text);
                requestAnimationFrame(() => speak(data.text));
            });

            socket.on('connect', () => {
                console.log('WebSocket connected:', socket.id);
            });

            socket.on('disconnect', () => {
                console.log('WebSocket disconnected');
            });
        }

        // Voice initialization
        synth.onvoiceschanged = () => {
            voicesReady = true;
            console.log('Voices loaded:', synth.getVoices());
        };

        // Initial setup
        document.addEventListener('DOMContentLoaded', () => {
            setupEventListeners();
            socket.connect();
        });
    </script>
</body>
</html>