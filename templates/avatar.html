<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>AI CHAT BOT</title>
    <link rel="icon" href="static/favicon.ico" type="image/x-icon">

    <script src="static/live2dcubismcore.min.js"></script>
    <script src="static/live2d.min.js"></script>
    <script src="static/pixi.min.js"></script>
    <script src="static/browser.js"></script>

    <script src="static/gsap.min.js"></script>
    <script src="static/socket.io.min.js"></script>
    <link rel="stylesheet" href="static/style.css">
</head>
<body>
    <canvas id="canvas"></canvas>

    <div class="controls">
        <div class="input-group">
            <textarea id="text-input" placeholder="Enter text for the avatar to speak..."></textarea>
            <button id="speakBtn">Speak</button>
        </div>
    </div>

    <script>
        // WebSocket configuration
        const socket = io({
            transports: ['websocket'],
            reconnectionAttempts: 5,
            timeout: 3000
        });

        // Speech synthesis setup
        const synth = window.speechSynthesis;
        let isSpeaking = false;
        let live2dModel = null;
        let voicesReady = false;
        let coreModel = null;
        var mouthMovement = null;

        const mouthState = { value: 0 };


        // Live2D model configuration
        const url = 'model/shizuku/shizuku.model.json';
        
        // Initialize application
        function main() {
            const app = new PIXI.Application({
                view: document.getElementById('canvas'),
                autoStart: true,
                resizeTo: window
            });

            app.ticker.add(() => {
                // mimic the interpolation value, 0-1
                if(isSpeaking)
                    mouthState.value = Math.sin(performance.now() / 200) / 2 + 0.5;
            });
        

            try {
                PIXI.live2d.Live2DModel.fromModelSettingsFile(url).then(model => {
                    app.stage.addChild(model);
                    
                    model.anchor.set(0.5, 0.5);
                    model.position.set(innerWidth / 2, innerHeight / 2);
                    
                    const size = Math.min(innerWidth, innerHeight) * 0.8;
                    model.width = size;
                    model.height = size;
                    
                    const updateFn = model.internal.motionManager.update;
                    
                    model.internal.motionManager.update = () => {
                        updateFn.call(model.internal.motionManager);
                        
                        // overwrite the parameter after calling original update function
                        model.internal.coreModel.setParamFloat('PARAM_MOUTH_OPEN_Y', mouthState.value);
                    }
                });

            } catch (error) {
                console.error('Model loading error:', error);
            }
        }


        // Speech functions
        function speak(text) {
            if (!text || !voicesReady) return;

            if (isSpeaking) {
                synth.cancel();
                isSpeaking = false;
                gsap.killTweensOf(mouthState);
            }

            const utterance = new SpeechSynthesisUtterance(text);
            const voice = synth.getVoices().find(v => v.name.includes('Female')) || synth.getVoices()[0];

            Object.assign(utterance, {
                voice: voice,
                pitch: 1.1,
                rate: 0.95
            });

            utterance.onstart = () => {
                isSpeaking = true;
                // Animate mouth using GSAP with sine wave pattern
                gsap.to(mouthState, {
                    duration: 0.2,
                    value: 1,
                    repeat: -1,
                    yoyo: true,
                    ease: "sine.inOut",
                    modifiers: {
                        value: () => Math.abs(Math.sin(performance.now() / 200)) * 0.8
                    }
                });
            };

            utterance.onend = () => {
                isSpeaking = false;
                gsap.killTweensOf(mouthState);
                gsap.to(mouthState, {
                    duration: 0.3,
                    value: 0,
                    ease: "power2.out"
                });
            };

            synth.speak(utterance);
        }

        // Event listeners
        function setupEventListeners() {
            document.getElementById('speakBtn').addEventListener('click', () => {
                const text = document.getElementById('text-input').value.trim();
                if (text) socket.emit('speak', { text });
            });

            socket.on('speak_text', data => speak(data.text));

            socket.on('connect', () => console.log('WebSocket connected:', socket.id));
            socket.on('disconnect', () => console.log('WebSocket disconnected'));
        }

        // Voice initialization
        synth.onvoiceschanged = () => {
            voicesReady = true;
            console.log('Voices loaded:', synth.getVoices());
        };

        // Initial setup
        document.addEventListener('DOMContentLoaded', () => {
            setupEventListeners();
            socket.connect();
            main();
        });
    </script>
</body>
</html>